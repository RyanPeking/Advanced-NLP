{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "assert tf.__version__.startswith('2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"D:\\data\\Q&A\\AutoMaster_TrainSet.csv\"\n",
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰GL级</td>\n",
       "      <td>方向机重，助力泵，方向机都换了还是一样</td>\n",
       "      <td>技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了...</td>\n",
       "      <td>随时联系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰M级</td>\n",
       "      <td>奔驰ML500排气凸轮轴调节错误</td>\n",
       "      <td>技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障...</td>\n",
       "      <td>随时联系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q3</td>\n",
       "      <td>宝马</td>\n",
       "      <td>宝马X1(进口)</td>\n",
       "      <td>2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地换挡位PRND车辆闯...</td>\n",
       "      <td>技师说：你好，4缸自然吸气发动机N46是吧，先挂空档再挂其他档有没有闯动呢，变速箱油液位是否...</td>\n",
       "      <td>行驶没有顿挫的感觉，原地换挡有闯动，刹车踩重没有，这是力的限制的作用，应该没有问题</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>牧马人</td>\n",
       "      <td>3.0V6发动机号在什么位置，有照片最好！</td>\n",
       "      <td>技师说：右侧排气管上方，缸体上靠近变速箱|车主说：[图片]|车主说：是不是这个？|车主说：这...</td>\n",
       "      <td>举起车辆，在左前轮这边的缸体上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q5</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰C级</td>\n",
       "      <td>2012款奔驰c180怎么样，维修保养，动力，值得拥有吗</td>\n",
       "      <td>技师说：家庭用车的话，还是可以入手的|技师说：维修保养费用不高|车主说：12年的180市场价...</td>\n",
       "      <td>家庭用车可以入手的，维修保养价格还可以。车况好，价格合理可以入手</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID Brand     Model                                           Question  \\\n",
       "0  Q1    奔驰     奔驰GL级                                方向机重，助力泵，方向机都换了还是一样   \n",
       "1  Q2    奔驰      奔驰M级                                   奔驰ML500排气凸轮轴调节错误   \n",
       "2  Q3    宝马  宝马X1(进口)  2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地换挡位PRND车辆闯...   \n",
       "3  Q4  Jeep       牧马人                              3.0V6发动机号在什么位置，有照片最好！   \n",
       "4  Q5    奔驰      奔驰C级                       2012款奔驰c180怎么样，维修保养，动力，值得拥有吗   \n",
       "\n",
       "                                            Dialogue  \\\n",
       "0  技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了...   \n",
       "1  技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障...   \n",
       "2  技师说：你好，4缸自然吸气发动机N46是吧，先挂空档再挂其他档有没有闯动呢，变速箱油液位是否...   \n",
       "3  技师说：右侧排气管上方，缸体上靠近变速箱|车主说：[图片]|车主说：是不是这个？|车主说：这...   \n",
       "4  技师说：家庭用车的话，还是可以入手的|技师说：维修保养费用不高|车主说：12年的180市场价...   \n",
       "\n",
       "                                      Report  \n",
       "0                                       随时联系  \n",
       "1                                       随时联系  \n",
       "2  行驶没有顿挫的感觉，原地换挡有闯动，刹车踩重没有，这是力的限制的作用，应该没有问题  \n",
       "3                            举起车辆，在左前轮这边的缸体上  \n",
       "4           家庭用车可以入手的，维修保养价格还可以。车况好，价格合理可以入手  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82943 entries, 0 to 82942\n",
      "Data columns (total 6 columns):\n",
      "QID         82943 non-null object\n",
      "Brand       81642 non-null object\n",
      "Model       81642 non-null object\n",
      "Question    82943 non-null object\n",
      "Dialogue    82941 non-null object\n",
      "Report      82873 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QID            0\n",
       "Brand       1301\n",
       "Model       1301\n",
       "Question       0\n",
       "Dialogue       2\n",
       "Report        70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 81572 entries, 0 to 82942\n",
      "Data columns (total 6 columns):\n",
      "QID         81572 non-null object\n",
      "Brand       81572 non-null object\n",
      "Model       81572 non-null object\n",
      "Question    81572 non-null object\n",
      "Dialogue    81572 non-null object\n",
      "Report      81572 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Whole_Q'] = data['Brand'] + data['Model'] + data['Question'] + data['Dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Report</th>\n",
       "      <th>Whole_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰GL级</td>\n",
       "      <td>方向机重，助力泵，方向机都换了还是一样</td>\n",
       "      <td>技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了...</td>\n",
       "      <td>随时联系</td>\n",
       "      <td>奔驰奔驰GL级方向机重，助力泵，方向机都换了还是一样技师说：[语音]|车主说：新的都换了|车...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰M级</td>\n",
       "      <td>奔驰ML500排气凸轮轴调节错误</td>\n",
       "      <td>技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障...</td>\n",
       "      <td>随时联系</td>\n",
       "      <td>奔驰奔驰M级奔驰ML500排气凸轮轴调节错误技师说：你这个有没有电脑检测故障代码。|车主说：...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q3</td>\n",
       "      <td>宝马</td>\n",
       "      <td>宝马X1(进口)</td>\n",
       "      <td>2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地换挡位PRND车辆闯...</td>\n",
       "      <td>技师说：你好，4缸自然吸气发动机N46是吧，先挂空档再挂其他档有没有闯动呢，变速箱油液位是否...</td>\n",
       "      <td>行驶没有顿挫的感觉，原地换挡有闯动，刹车踩重没有，这是力的限制的作用，应该没有问题</td>\n",
       "      <td>宝马宝马X1(进口)2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>牧马人</td>\n",
       "      <td>3.0V6发动机号在什么位置，有照片最好！</td>\n",
       "      <td>技师说：右侧排气管上方，缸体上靠近变速箱|车主说：[图片]|车主说：是不是这个？|车主说：这...</td>\n",
       "      <td>举起车辆，在左前轮这边的缸体上</td>\n",
       "      <td>Jeep牧马人3.0V6发动机号在什么位置，有照片最好！技师说：右侧排气管上方，缸体上靠近变...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q5</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰C级</td>\n",
       "      <td>2012款奔驰c180怎么样，维修保养，动力，值得拥有吗</td>\n",
       "      <td>技师说：家庭用车的话，还是可以入手的|技师说：维修保养费用不高|车主说：12年的180市场价...</td>\n",
       "      <td>家庭用车可以入手的，维修保养价格还可以。车况好，价格合理可以入手</td>\n",
       "      <td>奔驰奔驰C级2012款奔驰c180怎么样，维修保养，动力，值得拥有吗技师说：家庭用车的话，还...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID Brand     Model                                           Question  \\\n",
       "0  Q1    奔驰     奔驰GL级                                方向机重，助力泵，方向机都换了还是一样   \n",
       "1  Q2    奔驰      奔驰M级                                   奔驰ML500排气凸轮轴调节错误   \n",
       "2  Q3    宝马  宝马X1(进口)  2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地换挡位PRND车辆闯...   \n",
       "3  Q4  Jeep       牧马人                              3.0V6发动机号在什么位置，有照片最好！   \n",
       "4  Q5    奔驰      奔驰C级                       2012款奔驰c180怎么样，维修保养，动力，值得拥有吗   \n",
       "\n",
       "                                            Dialogue  \\\n",
       "0  技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了...   \n",
       "1  技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障...   \n",
       "2  技师说：你好，4缸自然吸气发动机N46是吧，先挂空档再挂其他档有没有闯动呢，变速箱油液位是否...   \n",
       "3  技师说：右侧排气管上方，缸体上靠近变速箱|车主说：[图片]|车主说：是不是这个？|车主说：这...   \n",
       "4  技师说：家庭用车的话，还是可以入手的|技师说：维修保养费用不高|车主说：12年的180市场价...   \n",
       "\n",
       "                                      Report  \\\n",
       "0                                       随时联系   \n",
       "1                                       随时联系   \n",
       "2  行驶没有顿挫的感觉，原地换挡有闯动，刹车踩重没有，这是力的限制的作用，应该没有问题   \n",
       "3                            举起车辆，在左前轮这边的缸体上   \n",
       "4           家庭用车可以入手的，维修保养价格还可以。车况好，价格合理可以入手   \n",
       "\n",
       "                                             Whole_Q  \n",
       "0  奔驰奔驰GL级方向机重，助力泵，方向机都换了还是一样技师说：[语音]|车主说：新的都换了|车...  \n",
       "1  奔驰奔驰M级奔驰ML500排气凸轮轴调节错误技师说：你这个有没有电脑检测故障代码。|车主说：...  \n",
       "2  宝马宝马X1(进口)2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地...  \n",
       "3  Jeep牧马人3.0V6发动机号在什么位置，有照片最好！技师说：右侧排气管上方，缸体上靠近变...  \n",
       "4  奔驰奔驰C级2012款奔驰c180怎么样，维修保养，动力，值得拥有吗技师说：家庭用车的话，还...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    import jieba\n",
    "    return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data.Whole_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81572"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = data.Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81572"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.756 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "questions_cut = ['<start> ' + cut(str(question)) + ' <end>' for question in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_cut = ['<start> ' + cut(str(answer)) + ' <end>' for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> 奔驰 奔驰 GL 级 方向机 重 ， 助力 泵 ， 方向机 都 换 了 还是 一样 技师 说 ： [ 语音 ] | 车主 说 ： 新 的 都 换 了 | 车主 说 ： 助力 泵 ， 方向机 | 技师 说 ： [ 语音 ] | 车主 说 ： 换 了 方向机 带 的 有 | 车主 说 ： [ 图片 ] | 技师 说 ： [ 语音 ] | 车主 说 ： 有 助力 就是 重 ， 这车 要 匹配 吧 | 技师 说 ： 不 需要 | 技师 说 ： 你 这 是 更换 的 部件 有 问题 | 车主 说 ： 跑 快 了 还好 点 ， 就 倒车 重 的 很 。 | 技师 说 ： 是 非常 重 吗 | 车主 说 ： 是 的 ， 累人 | 技师 说 ： [ 语音 ] | 车主 说 ： 我 觉得 也 是 ， 可是 车主 是 以前 没 这么 重 ， 选 吧 助理 泵 换 了 不行 ， 又 把 放 向 机换 了 ， 现在 还 这样 就 不 知道 咋 和 车主 解释 。 | 技师 说 ： [ 语音 ] | 技师 说 ： [ 语音 ] <end>',\n",
       " '<start> 随时 联系 <end>')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_cut[0], answers_cut[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers_cut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor): return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    sent_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    sent_tokenizer.fit_on_texts(sent)\n",
    "\n",
    "    tensor = sent_tokenizer.texts_to_sequences(sent)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, sent_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(num_examples, inp, targ):\n",
    "    inp = inp[:num_examples]\n",
    "    targ = answers_cut[:num_examples]\n",
    "    input_tensor, inp_sent_tokenizer = tokenize(inp)\n",
    "    target_tensor, targ_sent_tokenizer = tokenize(targ)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_sent_tokenizer, targ_sent_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_sent, targ_sent = load_dataset(300, questions_cut, answers_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397 141\n",
      "300 [[  4 514 515   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  4 514 515   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(max_length_inp, max_length_targ)\n",
    "print(len(input_tensor),target_tensor[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 240 60 60\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sent, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"%d ----> %s\" % (t, sent.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence; index to word mapping\n",
      "24 ----> <start>\n",
      "208 ----> 丰田\n",
      "902 ----> 汉兰达\n",
      "132 ----> 请问\n",
      "18 ----> 你\n",
      "1025 ----> 左\n",
      "1387 ----> 转向灯\n",
      "4 ----> ，\n",
      "18 ----> 你\n",
      "954 ----> 右\n",
      "1387 ----> 转向灯\n",
      "5 ----> 的\n",
      "411 ----> 速度\n",
      "63 ----> 要\n",
      "452 ----> 快\n",
      "1333 ----> 一半\n",
      "282 ----> 怎么回事\n",
      "11 ----> ？\n",
      "397 ----> 灯泡\n",
      "1807 ----> 没坏\n",
      "8 ----> 。\n",
      "7 ----> 技师\n",
      "1 ----> 说\n",
      "2 ----> ：\n",
      "31 ----> 你好\n",
      "4 ----> ，\n",
      "91 ----> 检查一下\n",
      "397 ----> 灯泡\n",
      "76 ----> 是不是\n",
      "372 ----> 烧\n",
      "10 ----> 了\n",
      "4 ----> ，\n",
      "94 ----> 或者\n",
      "397 ----> 灯泡\n",
      "2479 ----> 大小\n",
      "20 ----> 不\n",
      "130 ----> 一样\n",
      "73 ----> 大\n",
      "8 ----> 。\n",
      "3 ----> |\n",
      "7 ----> 技师\n",
      "1 ----> 说\n",
      "2 ----> ：\n",
      "397 ----> 灯泡\n",
      "2479 ----> 大小\n",
      "20 ----> 不\n",
      "130 ----> 一样\n",
      "4 ----> ，\n",
      "91 ----> 检查一下\n",
      "60 ----> 吧\n",
      "4 ----> ，\n",
      "93 ----> 有没有\n",
      "110 ----> 维修\n",
      "78 ----> 过\n",
      "90 ----> 那个\n",
      "312 ----> 灯\n",
      "5 ----> 的\n",
      "19 ----> 吗\n",
      "11 ----> ？\n",
      "3 ----> |\n",
      "6 ----> 车主\n",
      "1 ----> 说\n",
      "2 ----> ：\n",
      "980 ----> 真的\n",
      "2480 ----> 一切\n",
      "29 ----> 都\n",
      "55 ----> 正常\n",
      "172 ----> 着\n",
      "172 ----> 着\n",
      "852 ----> 放在\n",
      "534 ----> 那里\n",
      "4173 ----> 没动\n",
      "17 ----> 就\n",
      "71 ----> 出现\n",
      "50 ----> 这种\n",
      "44 ----> 故障\n",
      "10 ----> 了\n",
      "3 ----> |\n",
      "6 ----> 车主\n",
      "1 ----> 说\n",
      "2 ----> ：\n",
      "397 ----> 灯泡\n",
      "2480 ----> 一切\n",
      "29 ----> 都\n",
      "55 ----> 正常\n",
      "25 ----> <end>\n",
      "\n",
      "Target Sentence; index to word mapping\n",
      "4 ----> <start>\n",
      "263 ----> 看看\n",
      "302 ----> 那个\n",
      "255 ----> 灯泡\n",
      "489 ----> 想\n",
      "583 ----> 需\n",
      "1349 ----> 烧坏\n",
      "28 ----> 了\n",
      "1 ----> ，\n",
      "51 ----> 可能\n",
      "1350 ----> 有个\n",
      "255 ----> 灯泡\n",
      "714 ----> 差不多\n",
      "1351 ----> 要坏\n",
      "28 ----> 了\n",
      "3 ----> 。\n",
      "50 ----> 把\n",
      "1352 ----> 闪\n",
      "2 ----> 的\n",
      "478 ----> 快\n",
      "1353 ----> 那边\n",
      "18 ----> 检查一下\n",
      "27 ----> 是否\n",
      "1354 ----> 有要\n",
      "719 ----> 坏\n",
      "2 ----> 的\n",
      "28 ----> 了\n",
      "3 ----> 。\n",
      "5 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Sentence; index to word mapping\")\n",
    "convert(inp_sent, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Sentence; index to word mapping\")\n",
    "convert(targ_sent, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_sent.word_index) + 1\n",
    "vocab_tar_size = len(targ_sent.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5099, 1648)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 1397]), TensorShape([64, 141]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "#                                        return_sequences=True,\n",
    "#                                        return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform') # Xavier \n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "#         output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(self.gru(x, initial_state = hidden))\n",
    "        state = self.gru(x, initial_state = hidden)\n",
    "        return state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "state = encoder(example_input_batch, sample_hidden)\n",
    "# print(output.shape)\n",
    "# print(state_h.shape)\n",
    "# print(context_v.shape)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform') # Xavier \n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         output, state_h, context_v = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 1397, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# output, state_h, context_v = encoder(example_input_batch, sample_hidden)\n",
    "# print(output.shape)\n",
    "# print(state_h.shape)\n",
    "# print(context_v.shape)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    # other attention is LuongAttention\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query: hidden\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        print('query shape:(batch_size, units) {}'.format(query.shape))\n",
    "        print('values shape:(batch_size, sequence_length, units) {}'.format(values.shape))\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        print('self.W1 shape:(batch_size, units, attention_units) {}'.format(self.W1(values).shape))\n",
    "        print('self.W2 shape:(batch_size, 1, attention_units) {}'.format(self.W2(hidden_with_time_axis).shape))\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        print('score shape:(batch_size, sequence_length, 1) {}'.format(score.shape))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape:(batch_size, units) (64, 1024)\n",
      "values shape:(batch_size, sequence_length, units) (64, 1397, 1024)\n",
      "self.W1 shape:(batch_size, units, attention_units) (64, 1397, 10)\n",
      "self.W2 shape:(batch_size, 1, attention_units) (64, 1, 10)\n",
      "score shape:(batch_size, sequence_length, 1) (64, 1397, 1)\n",
      "(64, 1397, 1)\n",
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 1397, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 1608)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_sent.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:,t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:,t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function train_step at 0x0000019333720AE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function train_step at 0x0000019333720AE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000019330EB5F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000019330EB5F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001935D8C3B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001935D8C3B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000019330EB5F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000019330EB5F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000019332606588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x000001933258CBE0>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: hidden\n",
      "        # hidden shape == (batch_size, hidden size)\n",
      "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
      "        # we are doing this to perform addition to calculate the score\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        # score shape == (batch_size, max_length, 1)\n",
      "        # we get 1 at the last axis because we are applying score to self.V\n",
      "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
      "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "\n",
      "        # attention_weights shape == (batch_size, max_length, 1)\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "\n",
      "        # context_vector shape after sum == (batch_size, hidden_size)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence_english(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
